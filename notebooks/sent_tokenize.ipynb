{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280de425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.utils.text_utils import PageAwareSentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48fbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../pdfs/chen2017.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6c022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/Projects/complete-rag-project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.indexing import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cbce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = TextProcessor(1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540b48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextChunk(text='Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan Meta-evaluation of Online and Offline Web Search Evaluation Metrics Liu, Chen, Zhou, Zhang, Ma Ye Ke Yiqun Min Shaoping Tsinghua National Laboratory for Information Science and Technology, Department of Computer Science & Technology, Tsinghua University, Beijing, China University of Notingham, Notingham, U.K. yiqunliu@tsinghua.edu.cn ABSTRACT  Cranfield approach and are based on editorial judgments of the relevance of search results. Typical offline metrics include Average As in most information retrieval (IR) studies, evaluation plays an Precision (AP), Normalized Discounted Cumulative Gain (NDCG) essential part in Web search research. Both offline and online eval- and Rank-Biased Precision (RBP) . Tese metrics are widely uation metrics are adopted in measuring the performance of search used to measure the quality of ranking algorithms  and are engines.', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='Offline metrics are usually based on relevance judgments of great value in guiding search algorithm designing. However, of query-document pairs from assessors while online metrics ex- although they may provide easily interpretable outcomes, offline ploit the user behavior data, such as clicks, collected from search search evaluation has encountered two major problems. Te first engines to compare search algorithms. Although both types of IR one lies in that editorial judgments are ofen less credible when evaluation metrics have achieved success, to what extent can they measuring actual user experience. Recent studies show that asses- predict user satisfaction still remains under-investigated. To shed sors judgments may significantly differ from users assessments light on this research question, we meta-evaluate a series of existing .', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='Te second problem is that the evaluation results based on online and offline metrics to study how well they infer actual search offline metrics can be biased because they are usually generated user satisfaction in different search scenarios. We find that both with a small and incomplete dataset . types of evaluation metrics significantly correlate with user satis- Rather than relying on offline metrics with relevance judgments, faction while they reflect satisfaction from different perspectives a popular contrasting approach is to use online metrics based on the for different search tasks. Offline metrics beter align with user sat- simple fact that the interactions between users and search engines isfaction in homogeneous search (i.e. ten blue links) whereas online reflect the actual users experiences in a natural usage environment. metrics outperform when vertical results are federated.', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='Finally, we Such metrics are calculated based on practical users behavior logs also propose to incorporate mouse hover information into existing and can give us straightforward descriptions on how users interact online evaluation metrics, and empirically show that they beter with search engines. In addition, it is ofen cheap and fast to collect align with search user satisfaction than click-based online metrics. such data in modern search engines, making it particularly easy to scale up online evaluation. Typical online metrics include click- KEYWORDS  based metrics such as CTR (click through rate), UCTR (binary Search satisfaction, evaluation metrics, online evaluation value representing click) and PLC  (number of clicks divided by the position of the lowest click) as well as dwell time-based metrics such as query dwell time, average of click dwell time  1 INTRODUCTION and so on.', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='Although easily scalable and arguably more truthful Search engine evaluation is important in both academic and indus- indication of operational IR effectiveness, online metrics can suffer trial IR research. Te goal of IR researchers is to bulid search engine from various biases present in typical search logs. Online behavior systems which can satisfy users information needs. Both offline of users can be affected by many factors, with position bias being and online metrics have been adopted to measure how well the the most widely recognized effect, which requires de-biasing when system serves real users. Offline metrics mainly originated from inferring search success. In addition, online metrics may not be as reusable as offline metrics . Corresponding author Both online and offline metrics have been widely used to mea- Tis work was supported by Natural Science Foundation of China (Grant No. sure search performance in the past years.', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='Nonetheless, they are 61622208, 61532011, 61672311), Tsinghua University Initiative Scientific Research Pro- gram(2014Z21032), National Key Basic Research Program (2015CB358700). usually poorly correlated  because they measure IR systems from different perspectives. Which measures beter reflect the ulti- Permission to make digital or hard copies of all or part of this work for personal or mate actual user satisfaction remains an open research question. classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation Terefore, in this work we investigate the relationship between on the first page. Copyrights for components of this work owned by others than ACM offline/online metrics and actual user satisfaction, aiming to estab- must be honored. Abstracting with credit is permited.', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a lish a thorough understanding of the effectiveness of those metrics fee. Request permissions from permissions@acm.org. in various search scenarios. We meta-evaluate a series of existing SIGIR 17, August 07-11, 2017, Shinjuku, Tokyo, Japan evaluation metrics based on two public datasets with more than two  2017 ACM. ISBN 978-1-4503-5022-8/17/08...$15.00. DOI: htp://dx.doi.org/10.1145/3077136.3080804 15 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan metrics . Tey also adopted fuzziness value to examine the thousand search sessions. Qery-document relevance assessments, users interaction behaviors and their explicit satisfaction feedback power of a measure to discriminate among systems.', file_path='../pdfs/chen2017.pdf', page_num='1')\n",
      "TextChunk(text='Tis idea was are all included in the datasets, which makes us able to compute further formalized to be discriminative power by Sakai in 2006 most of the widely-used online and offline metrics. With more . He pointed out that mildly top-weighted metrics, such as than thirty metrics, we calculate Pearson correlation and conduct AP, NDCG and RBP(0.95) usually have higher discrimination ratios concordance test  to study how well each metric infers actual than those strongly-weighted metrics, such as Prec@5 and RBP(0.5) search user satisfaction. We found that while online and offline . In 2010, Yilmaz and Robertson compared AP and NDCG based on their statistical ability to predict outcomes on held-out data metrics measure users search experience from different perspec- tives, they generally both significantly correlate with actual user . Another criteria to compare metrics is to calculate the correla- satisfaction. tions between the system orderings generated by different metrics.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='Correlation coefficients such as Kendalls and Spearmans are To enable thorough meta-evaluation with different information   needs, we categorize the search tasks according to two existing widely used . However, a weakness is that such coefficients taxonomies, with respect to search goal types  and cognitive introduce the same penalty for the discords at different ranking level . We find that top-weighted offline metrics correlate ex- positions, whereas the top positions are usually more important in tremely well with user satisfaction in navigational search while  IR systems. To shed light on these considerations, Yilmaz et al. online metrics perform comparatively beter in informational and introduced an AP-based correlation coefficient called to achieve ap a top-weighted emphasis. Webber et al.  also proposed Rank transactional search tasks.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='Inspired by previous studies on federated [10, 32], Biased Overlap (RBO) to operate over indefinite and non-conjoint search we also compare the performance of evaluation met- rankings. Other evaluation criteria including evaluation based on rics in both homogeneous and heterogeneous search environment judgment cost [7, 8, 35, 49], coverage , inversions, interpreta- as users search behaviors as well as satisfaction perception may be affected by vertical results. We find that online metrics perform tion and volatility matrix were also proposed in the past few years beter than offline metrics in heterogeneous search environment. . Tis is probably because offline metrics mainly rely on relevance While A/B tests  and interleaving  as well as their vari- assessments while the interaction-based online metrics may be ants [30, 43] are also widely-used approaches for search system , more sensitive to the effect of vertical results.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='In addition, inspired evaluation we choose user satisfaction as the ground truth for by research on users fine-grained interaction behaviors such as evaluating different evaluation metrics because satisfaction reflects users search experience directly. Since the goal of IR systems is  , satisfied clicks and mouse hovers we also investigate the differences between online metrics calculated based on different to satisfy users information needs, it is important to investigate to what extent can existing evaluation metrics measure practical interaction behavior signals (clicks, satisfied clicks, hovers). Te re- users search experience. In this paper, we make a deep analysis sults show that online metrics can beter estimate user satisfaction of how different evaluation metrics correlate with user satisfac- when mouse hover information is incorporated. tion.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='Because satisfaction has been regarded as the gold standard Our contributions in this paper are three-folds: (1) We present a thorough meta-evaluation of online/offline metrics from the per- in search performance evaluation, we try to find metrics which spective of their relationship with user satisfaction for various can surrogate satisfaction, and introduce metrics which can beter types of information needs. Te results provide insights for both estimate user experience. evaluation metrics study and user satisfaction understanding. (2) 2.2 Search Satisfaction We investigate the differences and applicabilities of different eval- uation metrics in both homogeneous and heterogeneous search Search satisfaction has become one of the major concerns in search environment. We demonstrate that offline metrics work beter evaluation studies.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='Te concept of satisfaction was first proposed by in homogeneous search while online metrics outperform in het-  Su et al. and was defined as the fulfillment of a specified desire erogeneous search environment. (3) We propose to incorporate . or goal by Kelly To evaluate a search system, satisfaction can mouse hover information into existing online evaluation metrics be considered as regarding not only to the whole search experience and empirically demonstrate that they correlate beter with user but also to some specific aspects , such as the precision or satisfaction. completeness of search results, response time and so on. Since satisfaction is important for both search engine evaluation and 2 RELATED WORK optimization, a number of research studies have tried to quantify Of particular interest to our research is the extensive body of work [20, 47] user satisfaction in both desktop search and mobile search on (i) meta evaluation of IR metrics, and (ii) search satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='[26, 28],  and in both homogeneous and heterogeneous search 2.1 Meta Evaluation of IR Metrics environment . In recent years, a number of works (e.g, [21, 22]) have started using the benefit-cost framework to analyze the As evaluation serves as an important part in IR-related research, the satisfaction judgement process of users. In this framework, both meta-evaluation of evaluation metrics has also been widely studied in recent years and different criteria have been adopted to compare the benefit factors (document relevance) and cost factors (the effort different evaluation metrics . users spend on examining search engine result pages (SERPs) and One widely-used method is to use discriminative power to landing pages) are used to estimate satisfaction. Although satisfaction can be regarded as the gold standard in measure evaluation metrics.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='Early in 2000, Buckley and Voorhees proposed to use error rate, which is the likely error of concluding search performance evaluation, as mentioned, it is not easy to be col- System A is beter than system B, to compare between different lected. Tis makes it urgent to find a reliable and reusable metric to 16 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan label a satisfaction score to reflect his/her search experience. Ten estimate user satisfaction. However, as indicated by recent studies, they would be guided to continue to the next search task. relevance-based evaluation metrics, such as MAP and nDCG, may not be perfectly correlated with users search experience [2, 20]. Recently, Mao et al.  further studied the relationship between relevance, usefulness and satisfaction and also suggested that tradi- tional system-centric evaluation metrics are not well aligned with user satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='2')\n",
      "TextChunk(text='Different from these existing works, we study the relationships between user satisfaction and both offline and online evaluation metrics. Our work is complementary to those research on satis- faction understanding and prediction, but also provides additional insights from the evaluation measure perspective. We investigate how the existing metrics perform in different search scenarios and compare the applicabilities of different metrics in homogeneous and heterogeneous search environment. Meanwhile, we also in- Figure 1: Data Collection Procedure Note that no query reformulation are allowed and the number of vestigate how mouse hover information can be incorporated into search results are fixed to 10 for the consistency of result sets across online metrics to obtain beter alignment with user satisfaction. users. Such data collection setings are similar to previous studies Te major contribution of our work lies in that we comprehen- such as .', file_path='../pdfs/chen2017.pdf', page_num='3')\n",
      "TextChunk(text='Meanwhile, we adopt SERP-level satisfaction rather sively meta-evaluate over thirty most popular IR metrics for many than session-level satisfaction in this paper because most offline search tasks. Te obtained insights can be used for guiding search metrics are designed to measure the quality of only one search result evaluation practitioners. page. While there may be ways to adjust or merge the metrics to 3 DATASET AND METHODOLOGY measure session-level result quality, the metric adaptation may In this section, we describe the datasets as well as the meta-evaluation cause other uncontrollable effects and is out of the scope of this methods used throughout this paper. We also perform an analysis paper. of user satisfaction distribution according to the characteristics of Te search results in Dataset #1 are all organic search results the datasets. and the ones in Dataset #2 are mainly federated search results.', file_path='../pdfs/chen2017.pdf', page_num='3')\n",
      "TextChunk(text='Te vertical results included in Dataset #2 contain various types, includ- 3.1 Overview ing image, encyclopedia, news and download. Te combination of Dataset #1 and Dataset #2 is consistent with real-life setings Our study aims to meta-evaluate different metrics based on two available1. because not all SERPs provided by commercial search engines con- datasets which we have made publicly Tese two datasets tain vertical results. Furthermore, such composition of our datasets contain more than 2400 search sessions collected under 56 search also provides the advantage for us to evaluate how metrics perform search tasks in total. Te detailed statistics are shown in Table 1. differently in homogeneous and heterogeneous search environment Table 1: Characteristics of Datasets (see section 4.4).', file_path='../pdfs/chen2017.pdf', page_num='3')\n",
      "TextChunk(text='# different In these two datasets, each task is accompanied by several dif- # queries # users # sessions rankings per query ferent search result pages provided by several ranking mechanisms Dataset #1 26 3 40 1038 with a same pre-specified query, which is to ensure that all users Dataset #2 30 610 58 1397 saw the same page under the same ranking mechanism. Tis makes it possible for us to meta-evaluate the performance of different eval- Tese two datasets are generated under the same experimental uation metrics. Both datasets contain the following information for process which is shown in Figure 1. Each participant completed each search session: (1) Qery and corresponding task descriptions. a series of no more than 30 tasks in the datasets and they were (2) Information of ranked search results as shown on SERPs. (3) 4- required to perform two warm-up search tasks first to get familiar scaled relevance assessments of all search results. (4) 5-scaled user with the search process.', file_path='../pdfs/chen2017.pdf', page_num='3')\n",
      "TextChunk(text='Before each task, the participant was satisfaction annotations. (5) Users interaction behaviors during shown the search query and task explanations (see the card on the the search process, including click-through, mouse hover and dwell top right corner of Figure 1 as an example) first to avoid ambiguity. time information. Afer that, the participant would be guided to search result page With the rich information provided by the datasets, we can where the query is not allowed to change. Each participant was compute most widely-used offline / online metrics and hence meta- asked to examine the 10 fixed search results provided by the system evaluate the relationship between these metrics and users per- and end the search session either if the search goal was completed cieved satisfaction scores. or he/she was disappointed with the results.', file_path='../pdfs/chen2017.pdf', page_num='3')\n",
      "TextChunk(text='Te provided search result lists were pre-crawled from commercial search engines and 3.2 Search Task Taxonomy we made sure that the participant is able to complete the search goal as long as he/she examines all the provided search results. Each time To further evaluate the performance of different evaluation Metrics the participant completed a search session, he/she was required to in different search scenarios, we classify the search sessions into different categories based on the query and corresponding task de- 1Te link of the datasets and detailed data descriptions is scriptions provided in the dataset. We organize the search sessions htps://1drv.ms/f/s!AqRbaaorUiT1avvHQdagVv9uOPM 17 Session 1A:', file_path='../pdfs/chen2017.pdf', page_num='3')\n",
      "TextChunk(text='Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan Table 2: Examples of Search Qeries and Corresponding Taxonomies Qery Task Description Search Goal Cognitive Level Meizu official website Navigational Remember find the official website of Meizu find a biographical sketch of Stramaccioni Stramaccioni Informational Remember find the interview of Lee Sedol afer his match against AlphaGo interview of Lee Sedol Informational Understand find the online audition of yesterday once more sung by carpenters yesterday once more Transactional Remember find online videos about dunking dunk video Transactional Understand into the following two most widely-used task taxonomies in this paper: : Search Goal Tis taxonomy classifies search tasks from  the perspective of search goals. Te search queries are classified into navigational queries, informational queries and transactional queries. :', file_path='../pdfs/chen2017.pdf', page_num='3,4')\n",
      "TextChunk(text='Cognitive Level Tis taxonomy is proposed by Ander-  son and Krathwohl, which identifies six cognitive process dimensions: remember, understand, apply, analyze, evalu- ate and create. Table 2 shows an example set of the search tasks and their corre- sponding taxonomies while Table 3 presents the numbers of search queries / sessions within different types of search tasks. Note that (a) Taxonomy I we only include a subset of all the task types described in the task taxonomies due to the composition of our dataset. With respect to the cognitive level taxonomy, we only classify search sessions as either remember or understand since it is difficult and unreal- istic to classify the fixed 10 result based search sessions into the other four types of search tasks. Te data size of the navigational search tasks is comparatively small.', file_path='../pdfs/chen2017.pdf', page_num='4')\n",
      "TextChunk(text='Although the meta-evaluation results may be potentially less reliable for this navigational search task, we believe this can still provide useful preliminary insights while the more thorough analysis with more data points are lef for future work. Table 3: Distribution of Qeries and Sessions of Different Types of Search Tasks (b) Taxonomy II Navigational Informational Transactional Figure 2: Satisfaction Distribution Based on Different Search Qeries 10 23 23 Task Taxonomies with Different Qintiles Sessions 400 1047 988 task categories. We can see that in all types of search tasks, the Remember Understand general trend is that users tend to give a high satisfaction score Qeries 31 25 in most cases, which indicates that commercial search engines Sessions 1325 1110 generally provide promising results for these non-long-tailed search tasks.', file_path='../pdfs/chen2017.pdf', page_num='4')\n",
      "TextChunk(text='3.3 Analysis of User Satisfaction With respect to the first taxonomy based on query intent, we In this section, we try to compare the satisfaction distribution can see that users tend to be the least satisfied if they are searching across different search task taxonomies. Inspired by , which with informational queries because the percentage of sessions with says satisfaction judgement may be quite subjective and different lower Z-scores (less than 60%) is comparatively higher in the infor- users may have different opinions, we regularize the satisfaction mational case (35.4%) than in navigational (29.0%) and transactional scores labelled by each user into Z-scores according to equation (1), (18.9%) cases. Also, the percentage of sessions of the highest 20 where is one particular satisfaction score given by one user and percent of Z-scores is extremely low (1.3%) for informational search sati is the average of all satisfaction scores he/she labelled.', file_path='../pdfs/chen2017.pdf', page_num='4')\n",
      "TextChunk(text='tasks compared with the other two types of search tasks. Tis is Av(Sat) in equation (1) refers to the variance of the satisfaction reasonable because in most navigational and transactional search (Sat) Var scores of this user. tasks, users intended to find a specific website or information re- Av(Sat) sati source, which can ofen be satisfied with only one search result. Z-scorei = (1) (Sat) Var While in the case of informational search tasks, users ofen have to read quite a number of search results to get a comprehensive Figure 2 shows the distribution of quintiles in increasing Z-scores based on different search task taxonomies. Different colors show understanding of the information need, which may be more difficult satisfaction scores from search sessions originated from different and time consuming.', file_path='../pdfs/chen2017.pdf', page_num='4')\n",
      "TextChunk(text='18 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan Table 4: Numbers of data points / pairs for meta-evaluation = 0; = 0; = 0; Total Correct1 Correct2 # data points for # data pairs for pair of search sessions (s1,s2) foreach do Pearson Correlation Concordance Test + +; Total All 291 744 = M(s2); M(s1) M Navigational 30 30 M(s2); M= M(s1) Informational 130 348 (MM)> 0 Mpositively if then Transactional 131 366 // and agree M + +; Correct1 Remember 152 378 (MM)< 0 Mnegatively if then Understand 139 366 // and agree M + +; Correct2 Homogeneous 78 78 M= (M = 0 and 0) Magree if then // and Heterogeneous 213 666 M + +; Correct1 + +; Correct2 We believe our results (shown in Sec. 4) are still informative and = Max(Correct1,Correct2) Correct evaluation on a larger dataset can be carried out in the future.', file_path='../pdfs/chen2017.pdf', page_num='4,5')\n",
      "TextChunk(text='M) Concordance(M, = Correct/Total; Algorithm 1: Computing the concordance of metrics M 4 EXPERIMENTAL RESULTS M(user and golden standard metric satisfaction feedback) In this section, we meta-evaluate the performance of different eval- based on preference agreement. uation metrics in different search scenarios based on the algorithm described in section 3.4 to obtain thorough insights into how dif- ferent metrics perform according to different information needs. We first evaluate the performance of offline metrics and online From the perspective of the second task taxonomy based on metrics in section 4.1 and 4.2, respectively. In section 4.3, we take a task cognitive level, we can see that users give lower satisfaction deep insight into how some main offline/online metrics infer user scores in search tasks which belong to understand categories. satisfaction in different search scenarios. Finally, we incorporate Tis is in line with our expectation because search tasks identified mouse', file_path='../pdfs/chen2017.pdf', page_num='5')\n",
      "TextChunk(text='hover information into some existing online metrics and as understand category are considered to be more difficult than demonstrate its effectiveness in section 4.4. those identified to be remember tasks. In general, from the results shown in Figure 2, we can see that users perceive different levels 4.1 Comparison Across Offline Metrics of satisfaction in different search scenarios, which inspires us to study the relationship between different evaluation metrics and Table 5: Comparison of Pearson Correlations / Concordance satisfaction across different search task taxonomies.', file_path='../pdfs/chen2017.pdf', page_num='5')\n",
      "TextChunk(text='between Satisfaction and Offline Metrics (* indicates t-test < 0.01 statistical significance at level) p 3.4 Meta-Evaluation Methods Pearson Correlation Concordance With satisfaction widely regarded as the gold standard of user- CG 0.354* 45.8% centric evaluation metrics, we analyze which metrics can beter DCG@3 0.356* 61.6%* DCG@5 0.411* 65.7%* reflect user satisfaction based on the datasets described in this DCG@10 0.421* 65.3%* section. We do not consider session-based SAT in our work, rather AP 0.396* 60.2%* we assume one SERP page interaction, which consists of most of the RBP(0.1) 0.389* 66.7%* search sessions. With respect to the meta-evaluation methodology, RBP(0.5) 0.438* 66.5%* we use both pearson correlation coefficient  and concordance RBP(0.8) 65.7%* 0.445* test to compare different evaluation metrics. Te idea of using RBP(0.95) 0.384* 63.4%* concordance test is inspired by  and the method is described ERR 0.433* 66.8%* in Algorithm 1.', file_path='../pdfs/chen2017.pdf', page_num='5')\n",
      "TextChunk(text='We use and to denote different systems of s1 s2 the same task in our dataset and to denote the averaged Based on the search result relevance assessments provided by the M(si ) value of metric computed on all sessions under si. Te gold datasets, we compute some widely-used traditional offline metrics M Mis and investigate how they align with user satisfaction, including standard metric user satisfaction. Our algorithm differs from the algorithm used in  in that we take the possibility of both cumulative gain (CG), discounted cumulative gain (DCG), average positive and negative correlation into consideration. Furthermore, precision (AP) and rank-biased precision (RBP). For DCG, we com- we use strict and instead of and in the concordance test > < pare the performance of the metric calculated at different ranking lengths to investigate the effect of evaluation depth. For RBP, we  because loose restrictions cannot work properly for two-valued metrics such as UCTR.', file_path='../pdfs/chen2017.pdf', page_num='5')\n",
      "TextChunk(text='compare the metric performance when the persistence parameter p Te number of data points for computing pearson correlation is set as 0.1, 0.5, 0.8 and 0.95, which is suggested to be appropriate , for impatient, neutral, patient and extremely patient users re- and data pairs for concordance test in different search scenarios are shown in Table 4. We should note that the size of data points/pairs spectively. We also include expected reciprocal rank (ERR), which is of navigational search and homogeneous search (due to the lim- based on the cascade user model and is suggested to beter corre- ited ranking mechanisms of Dataset #1) is comparatively small, lates with click-based metrics compared to DCG and other editorial metrics . Te pearson correlation coefficients and concordance which may lead to the insignificant results in these two taxonomies.', file_path='../pdfs/chen2017.pdf', page_num='5')\n",
      "TextChunk(text='19 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan Mouse-based (clicks or scroll) metrics test results between these metrics and user satisfaction are shown  in Table 5. Te best pearson correlation and concordance results - Binary variable indicating whether there was  UCTR are bolded. a click or not in the session (the opposite of abandon- ment). We can see that ERR achieves the highest concordance with user satisfaction based on the results in Table 5, which is in line with - Number of clicks in a session.   QCTR the findings in . Tis may be because the cascade user model . - Page click-through rate as defined in We  PCTR use all clicks rather than satisfied clicks to calculate utilized in ERR beter models user behavior that capture satisfac- PCTR in Table 6, which is different from  where tion. RBP(0.8) achieves the highest pearson correlation among all only satisfied clicks are used.', file_path='../pdfs/chen2017.pdf', page_num='5,6')\n",
      "TextChunk(text='Comparisons between the metrics, which may indicate that a patient (but not extremely patient) rank-biased user model can best describe the character- metrics based on other user interaction signals (e.g. istic of the tested user group. From the perspective of DCG, we satisfied clicks) are further discussed in section 4.3. can see that DCG@10 correlates user satisfaction slightly beter - Respectively maximum,  MaxRR, MinRR, MeanRR minimum and mean reciprocal ranks of the clicks. than DCG@3 and DCG@5, which probably indicates that metrics calculated based on a longer ranking length can capture more in- Zero if no clicks. formation and may have a beter estimation of user satisfaction. - Number of clicks divided by the position of the  PLC lowest click. Among all these offline metrics, CG has the lowest pearson correla- - Maximum of scroll distance. tion and concordance with user satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='6')\n",
      "TextChunk(text='Tis is due to the fact  MaxScroll that DCG, AP as well as ERR are all top-weighted metrics while Dwelltime-based metrics  CG is not. Relevant results placed at top positions may be much - Respectively sum  SumClickDwell, AvgClickDwell more important than those placed at botom. Futhermore, we can and average of click dwell time in a query. note that the poor performance of CG is especially remarkable in - Qery dwell time.   QeryDwellTime - Time delta the case of navigational search scenario (shown in Table 7) where  TimeToFirstClick, TimeToLastClick between the start of search session and the first click one top-ranked relevant result is usually sufficient to complete the search goal. and last click in the session, respectively. Overall, we can observe that many offline metrics (DCG, RBP - Previous stud-  DsatClickCount, DsatClickRatio ies divide clicks into satisfied clicks and dissatisfied ) and ERR) have significant and moderate correlations (0.4 to 0.6 with user satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='6')\n",
      "TextChunk(text='[17, 21]. clicks based on various dwell time thresholds We tested different thresholds and choose to define 4.2 Comparison Across Online Metrics <15s clicks with a dwell time as dissatisfied clicks be- cause it performs the best on our dataset. Previous Table 6: Comparison of Pearson Correlations / Concordance  work also analyzed the threshold of 15s to differ- between Satisfaction and Online Metrics (* indicates t-test entiate satisfied clicks. We calculate the number and < 0.01 statistical significance at level) p ratio of dissatisfied clicks, respectively. Pearson Correlation Concordance Te online metrics we discuss in this section are mostly based UCTR -0.069 24.2%* on mouse (click and scroll) behaviors and dwell time information, QCTR -0.330* 57.9%* which can be easily computed based on users behavior logs. Tere PCTR@3 0.043 50.3%* are also several studies tried to utilize users behavior information to PCTR@5 -0.092 44.5%* [1, 18]).', file_path='../pdfs/chen2017.pdf', page_num='6')\n",
      "TextChunk(text='We do not include quantify or predict user satisfaction (e.g. PCTR@10 -0.226* 33.5%* these methods in our work because they are more complicated MaxRR 0.095 50.1%* prediction models, rather than the simple and easy-to-interpret MinRR 0.330* 61.2%* evaluation metrics of our interests. Meanwhile, we do not include MeanRR 0.266* 59.5%* the session-based metrics discussed in [17, 24] because there are PLC 0.222* 58.1%* MaxScroll 60.9%* no query reformulations included in the datasets. -0.519* Te correlations / concordances between the online evaluation SumClickDwell -0.417* 58.9%* metrics and user satisfaction are shown in Table 6, whereas the AvgClickDwell -0.109 50.9%* highest correlation based on each interaction signals are shown in QeryDwellTime 62.6%* -0.559* TimeToFirstClick -0.432* 65.6%* bolded terms. Te results in reveal a number of interesting findings: TimeToLastClick -0.504* (1) In contrast to the positive correlation between offline met- 67.3%* DsatClickCount -0.170', file_path='../pdfs/chen2017.pdf', page_num='6')\n",
      "TextChunk(text='* 56.0%* rics and user satisfaction, online metrics generally correlates with DsatClickRatio -0.130* 52.3%* user satisfaction negatively. Tis is reasonable because the offline metrics measure the quality of search result page based on rele- While offline metrics are especially valuable when evaluating a vance assessments and users usually tend to feel more satisfied if [13, 34], system in prior to its deployment online metrics have been widely adopted for modern search engines because such metrics the search results are of high quality . On the contrary, the interactions signals which online metrics adopted usually reflects are calculated based on the interactions between practical users and search effort and high search effort can reduce user satisfaction systems. Inspired by previous research on metrics meta-evaluation [9, 11, 15, 19], we compare the evaluation performance of some [10, 21]. MaxScroll also correlates with satisfaction negatively be-', file_path='../pdfs/chen2017.pdf', page_num='6')\n",
      "TextChunk(text='most widely-used online metrics, including: cause a longer scroll distance may also indicate more search effort. 20 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan MaxRR, MinRR and MeanRR compute the reciprocal ranks of the Te highest correlation achieved by offline metrics/online metrics clicked results and hence correlate with satisfaction positively. It in each task taxonomy are in bolded terms. Te results in Table 7 show that generally online metrics have is in line with the findings in previous studies that PLC correlates with satisfaction positively as PLC is regarded as approximately as good correlation and concordance as offline metrics, which fur- ther verifies the value of using online metrics in guiding search the precision of examined results . (2) Te metrics based on click behaviors in general correlates engine development because they achieve comparatively good per- formance without external relevance assessments.', file_path='../pdfs/chen2017.pdf', page_num='6')\n",
      "TextChunk(text='In most task more weakly with user satisfaction, compared with dwelltime-based scenarios, RBP and ERR perform the best among all offline metrics metrics. Tis may be because a clicked result does not always neces- while TimeToLastClick along with QeryDwellTime perform the sarily mean a high quality document hence the click-based metrics may fail. Meanwhile, previous studies [37, 43] pointed out that best among all online metrics, which is consistent with the findings approximately one order of magnitude more online samples are in section 4.1 and 4.2. required to match corresponding offline metrics reliability, which From the search goal-based task taxonomy, we should note that top-weighted offline metrics correlate quite well with user satis- may also explain the reason of the comparatively poor performance of click-based metrics.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='In contrast, some metrics based on scroll faction in navigational search tasks, especially for RBP(0.1) which (MaxScroll) and dwelltime information (SumClickDwell, QeryD- models the search behavior of impatient users (strong pearson cor- relation, 0.6 to 0.8 ). While the concordance test results may wellTime and TimeToLastClick ) have stronger (moderate) negative be discrete and not so reliable due to the limited number of data correlation with user satisfaction, which means scrolls and dwell- time information are quite important behavior signals to infer user pairs, the pearson correlation coefficients are extremely significant. Tis is because in navigational search scenario, where the user is satisfaction. (3) Among all these online metrics, TimeToLastClick has the usually required to reach a particular site , high quality results placed at the top positions are especially important. In contrast, best concordance with user satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='Te last click in a search session is usually considered as satisfied click  and therefore online metrics perform slightly worse in navigational tasks than in TimeToLastClick measures the time wasted before the user find informational and transactional tasks. Tis is probably because the search effort required in navigational search is usually less than a satisfactory document, which may account for the good perfor- that required in informational and transactional search, in which mance of TimeToLastClick. We can also note that the concordance between TimeToLastClick and user satisfaction is even beter than case the online metrics can hardly capture the differences of users satisfaction perception. the offline metrics, which implies that online metrics can be as avail- able as offline metrics even without offline relevance judgments.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='We do not observe too much difference of the performance of different metrics from the perspective of cognitive level based tax- QeryDwellTime has the strongest (moderate) pearson correlation but relatively low concordance with user satisfaction. Tis may be onomy. Te dwelltime based online metrics perform slightly beter because query dwell time has the largest value range among all in understand search scenario but the difference is not remarkable. Such findings may suggest that users do not behave significantly these metrics, which may take an advantage during the computing different in such two types of search tasks. An analysis in tasks process of pearson correlation.   with more deep cognitive levels such as analyze and create (4) From the perspective of ranking length, we can observe that PCTR@10 correlates user satisfaction beter than PCTR@3 and can be carried out in the future. We also observe the poor concordance of PCTR.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='Tis is because PCTR@5, which is consistent with the findings of DCG and further the test is conducted within the same task across different SERPs indicates that metrics calculated based on a longer ranking length can beter estimate user satisfaction. We can also note that PCTR generated by different ranking mechanisms. Te results on different has very low concordance with user satisfaction compared with SERPs are the same in most cases while the rankings are different. other metrics. Tis is because there are quite a number of SERPs Terefore, there might exist many identical metric PCTR values for with the same PCTR metric values in our dataset while there are the two SERPs of many data pairs while users perceived satisfaction is different. According to Algorithm 1, such case will be regarded minor changes within the satisfaction judgements.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='Terefore, this as discordance, which is the reason for the poor concordance of can result in discordance according to Algorithm 1. (5) It is in line with our expectation that there is almost no PCTR. While most of the results in Table 7 are informative, we correlation between user satisfaction and simple metrics such as must admit the comparatively small-scaled dataset in Navigational UCTR. UCTR also has a very poor concordance with satisfaction, search scenarios is an inevitable limitation. A small number of data pairs for concordance test may result in discrete and unavailable which is because it is a two-valued metric while we require strict < or > in Algorithm 1. results. We may need a larger dataset for more robust results in the future. In general, we can observe that several online metrics (MaxScroll, QeryDwellTime,TimetoFirstClick and TimeToLastClick) maintain 4.4 Metric Evaluation in Homogeneous / significant and moderate correlations (0.4 to 0.6) with satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='Heterogeneous Search 4.3 Online Metrics v.s. Offline Metrics With data collected from both homogeneous search environment Based on the findings in section 4.1 and 4.2, we select out some (Dataset #1) and heterogeneous search environment (Dataset #2), we investigate how different metrics perform in different search well-behaved metrics and investigate their correlations with user satisfaction in different task taxonomies described in section 3.2 to environments as shown in Table 8. Te highest correlation achieved by different metrics in different search scenarios are bolded. make a more detailed comparison.', file_path='../pdfs/chen2017.pdf', page_num='7')\n",
      "TextChunk(text='Te results are shown in Table 7. 21 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan Table 7: Comparison of Pearson Correlations / Concordance between Satisfaction and Offline and Online Metrics in Different < 0.01 Search Scenarios(* indicates t-test statistical significance at level) p Search Goal Cognitive Level Navigational Informational Transactional Remember Understand CG 0.335 / 53.3% 0.405* / 47.1% 0.354* / 44.0%* 0.414* / 44.7%* 0.389* / 47.0%* DCG@10 0.543* / 76.7%* 0.454* / 64.1%* 0.403* / 65.6%* 0.475* / 67.5%* 0.437* / 63.1%* RBP(0.1) / 0.400* / 66.4%* 0.314* / 0.407* / 68.0%* 0.371* / 65.3%* 0.653* 80.0%* 65.8%* RBP(0.8) 0.566* / / 65.8%* / 64.5%* / 66.7%* / 64.8%* 80.0%* 0.475* 0.419* 0.492* 0.454* ERR 0.625* / 0.451* / 0.348* / 0.432* / 0.430* / 80.0%* 66.7%* 65.8%* 68.0%* 65.6%* QCTR -0.187 / 53.3% -0.345* / 58.0%* -0.290* / 58.2%* -0.272* / 58.2%* -0.367* / 57.7% *', file_path='../pdfs/chen2017.pdf', page_num='8,7')\n",
      "TextChunk(text='PCTR@10 -0.135 / 0.0%* -0.358* / 33.9%* 0.018 / 35.8%* -0.005 / 33.6%* -0.392* / 33.3%* MinRR 0.454* / 0.323* / 61.5%* 0.281* / 59.8%* 0.318* / 59.8%* 0.344* / 62.6%* 73.3%* PLC 0.408* / 70.0%* 0.195* / 57.2%* 0.181* / 57.9%* 0.242* / 57.7%* 0.213* / 58.5%* MaxScroll / -0.577* / 59.2%* -0.465* / 61.5%* -0.471* / 63.2%* -0.547* / 58.5%* -0.477* 73.3%* QeryDwellTime -0.392* / 50.0% / 62.1%* / 64.2%* / 64.6%* -0.572* / 60.7%* -0.600* -0.479* -0.485', file_path='../pdfs/chen2017.pdf', page_num='8')\n",
      "TextChunk(text='* TimeToLastClick -0.351 / 50.0% -0.525* / -0.428* / -0.445* / / 68.1%* 68.0%* 67.5%* -0.551 67.2%* DsatClickCount -0.020 / 53.3% -0.187 / 55.7%* -0.145 / 56.6%* -0.134 / 56.1%* -0.223* / 56.0%* Table 8: Comparison of Pearson Correlations / Concordance between Satisfaction and Offline and Online Metrics in Homoge- < 0.01 neous and Heterogeneous Search Environment(* indicates t-test statistical significance at level) p homogeneous search heterogeneous search Pearson Correlation Concordance Pearson Correlation Concordance CG 0.483* 55.1% 0.321* 44.7% DCG@10 70.5%* 0.392* 64.7%* 0.535* Offline Metrics RBP(0.1) 0.433* 0.383* 66.1%* 71.8%* RBP(0.8) 0.418* 65.0%* 0.535* 71.8%* ERR 0.462* 71.8%* 0.429* 66.2%* QCTR -0.137 55.1% -0.401* 58.3%* PCTR@10 -0.062 2.6%* -0.284* 37.4%* Online Metrics MinRR 0.417* 0.325* 64.1%* 60.8%* (Mouse-based) PLC 0.366* 61.5%* 0.176* 57.7%* MaxScroll 60.5%* -0.510* 64.1%* -0.540* QeryDwellTime -0.224* 65.0%* 57.7% -0.637* Online Metrics', file_path='../pdfs/chen2017.pdf', page_num='8')\n",
      "TextChunk(text='TimeToLastClick 50.0% -0.570* -0.235* 69.4%* (Dwelltime-based) DsatClickCount -0.034 53.8% -0.209* 56.3%* Different metrics are categorized into three groups and it is ob- ), previous work the number of clicks in heterogeneous search vious from Table 8 that different groups of metrics reveal different may not be sufficient and hence may lead to a drop of performance characteristics in different search environments. Offline metrics of click-based online metrics. In our datasets, there are on average 2.83 clicks in a homogeneous search session while only 1.81 clicks beter align with user satisfaction in homogeneous search environ- in a heterogeneous one. Tis may be the reason that click-based ment while in heterogeneous search tasks, online metrics, especially the dwelltime-based metrics perform much beter. Tis is reason- online metrics do not perform so well in heterogeneous search.', file_path='../pdfs/chen2017.pdf', page_num='8')\n",
      "TextChunk(text='able because most existing offline metrics do not take the effect of 4.5 Click and Hover based Online Metrics vertical results into consideration during the evaluation process. [10, 16] Previous research suggested that fine-grained user interac- While both organic and vertical search results can provide relevant tion information can help beter model user behaviors and estimate information, vertical results are presented in different styles and user satisfaction. With the rich interaction informational provided can help satisfy users information need from various dimensions by the datasets, we further investigate the performance of online .', file_path='../pdfs/chen2017.pdf', page_num='8')\n",
      "TextChunk(text='Offline metrics are solely based on relevance assessments and metrics calculated based on the following four types of user inter- do not consider the effect of vertical results, which may account action signals: for their comparatively poor performance in heterogeneous search click-based: Tis group of evaluation metrics are calculated All environment. Online metrics are mainly based on users interaction based on all clicks in a search session, which is the same as the behaviors, which may also be sensitive to the existence of vertical metrics used in the previous sections. results. Terefore, the online metrics may be more effective in click-based: Previous studies pointed out that some- Satisfied the heterogeneous search environment. Dwelltime-based metrics times clicks do not imply the high relevance of a result document achieves a pearson correlation of around -0.6 (strong) in hetero- in web search.', file_path='../pdfs/chen2017.pdf', page_num='8')\n",
      "TextChunk(text='Tis is because although the result snippet may geneous search while the best offline metrics is only moderately appear to be relevant and atractive, the landing page may be of correlated (around 0.4). In addition, note that since high quality ver- low quality. For example, a click is defined as a satisfied click if the tical results can provide users sufficient information to complete the user spent 30 seconds or more reading the clicked document or if it search goal without a need to click (called good abandonment in . was the last click in the search session Satisfied click is widely 22 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan regarded as the signal of a relevant document. For this group of rank position of search results, we compute the percentage of ses- sions with clicks and hovers respectively. It is apparent that there metrics, we use satisfied clicks to replace the all clicks used in all click-based metrics.', file_path='../pdfs/chen2017.pdf', page_num='8')\n",
      "TextChunk(text='are more hovers than clicks in all positions, which may further help confirm that clicks solely may not be sufficient to capture users Hover-based: Hovers are also regarded to be important behav- ior signal because there have been various types of results which interaction information. Tere appears to be at least 27% sessions do not require a click to provide users with necessary informa- with hovers in all positions while only the first two positions have tion [10, 29]. We use hovers to replace the all clicks used in all a probability of more than 27% to be clicked. In this way, hovers may contain much more valuable information than clicks. From click-based metrics to achieve the hover-based metrics. Hover-based: We combine the click and hover in- the perspective of MinRR and PLC, a metric which combines click Click and formation in this group of evaluation metrics.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='If a search results and hover information may be the most reliable because neither click nor hover information alone (PLC h) can achieve the best is either clicked or hovered on, then this result will be regarded as clicked as in the calculation process of all click-based metrics. correlation with satisfaction. In this way, we get the click and hover-based metrics. 5 CONCLUSIONS Search engine evaluation is essential in both academic and indus- trial IR research. Both offline and online evaluation metrics are adopted to measure the performance of search engines. While search satisfaction is widely regarded as the gold standard in search performance evaluation, the relationship between different evalua- tion metrics and satisfaction remains under-investigated. In this work, we meta-evaluate the performance of different offline/online evaluation metrics based on two datasets.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='We inves- tigate how different metrics align with user satisfaction in different search scenarios using both pearson correlation and concordance test. We find that different types of evaluation metrics estimate user satisfaction from different perspectives. Offline metrics work beter in homogeneous search environment while online metrics Figure 3: Percentage of all search sessions with clicks/hovers are more consistent with user satisfaction in heterogeneous search at different result positions environment. We further compare the effectiveness of metrics cal- We use suffix  ac,  sc,  h and  ch to represent the all culated based on different user interaction signals. We propose to click-based, satisfied click-based, hover-based and click and incorporate hover information into traditional click-based online hover-based metrics, respectively. We choose MinRR and PLC as metrics because they can help beter estimate user satisfaction.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='examples because they correlate with user satisfaction beter than Tere are still some limitations of our work which we would like other click-based metrics based on the findings in previous sections. to list as our future work directions. Due to the nature of the uti- Te correlations between user satisfaction and metrics computed lized laboratory-based datasets, compared to the commercial search based on different information signals are shown in Table 9. Te engine setings, online metrics are calculated based on relatively best correlation / concordance achieved by each metric in different small-scale search sessions (from which our conclusions are drawn). search scenarios are bolded. Meanwhile, our datasets are based on a fixed search result page and We can see from the results that in almost all search scenarios, no ability to reformulate the query, which may also affect users in- both of these two click-based metrics can beter estimate user satis- teraction behaviors.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='While metrics calculated based on multi-query faction when hover information is incorporated. Tis is probably sessions can be developed, it will be another challenging research because in todays search engine, various types of results such as question. Our current aim is to meta-evaluate all the metrics on instant answers, verticals and even result snippets contain sufficient a single search page level, and we leave the session-level or even information to satisfy the users, which sometimes makes clicks un- task-level evaluation as future work. Finally, all the participants necessary. In such case, hovers can help capture more information within the datasets are undergraduate students. We think this may than clicks. It is not surprising that the performance of MinRR h help reduce potential distractions and make the collected data more and MinRR ch are the same because in most cases the clicked re- consistent.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='However, such specific age distribution may also cause sults are usually a subset of hovered results. Furthermore, we can potential bias. Te study of large-scale online/offline metrics com- see that the hover information is especially effective in heteroge- parison, online metric sensitivity and more evaluation measures neous search according to the performance of MinRR. When hover (such as interleaving) are lef for future work. information is incorporated, the pearson correlation coefficient REFERENCES improves by 0.169(from 0.325 to 0.494) in heterogeneous search and only 0.025(from 0.417 to 0.442) in homogeneous search. Te im-  Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene Agichtein. 2011. Find it if you can: a game for modeling different types of web search success using interaction provement of concordance test result is also larger in heterogeneous data. In SIGIR11. ACM, 345354. search environment.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='Tis is reasonable because sometimes users Azzah Al-Maskari, Mark Sanderson, and Paul Clough. 2007. Te relationship  between IR effectiveness measures and user satisfaction. In SIGIR07. ACM, 773 can accomplish their search tasks by interacting with the vertical 774. results on heterogeneous SERPs. An example of the distribution dif- Lorin W Anderson, David R Krathwohl, and Benjamin Samuel Bloom. 2001. A  ferences between hovers and clicks are shown in Figure 3.', file_path='../pdfs/chen2017.pdf', page_num='9')\n",
      "TextChunk(text='For each taxonomy for learning, teaching, and assessing: A revision of Blooms taxonomy of 23 Session 1A: Evaluation 1 SIGIR17, August 7-11, 2017, Shinjuku, Tokyo, Japan Table 9: Comparison of Pearson Correlations / Concordance between Satisfaction and Online Metrics Based on Different User < 0.01 Interaction Signals (* indicates t-test statistical significance at level) p Search Goal Cognitive Level Search Environment Navigational Informational Transactional Remember Understand Homogeneous Heterogeneous MinRR ac 0.454* / 73.3%* 0.323* / 61.5%* 0.281* / 59.8%* 0.318* / 59.8%* 0.344* / 62.6%* 0.417*/64.1%* 0.325*/60.8%* MinRR sc 0.442* / 0.322* / 0.209* / 58.2%* 0.254* / 58.7%* 0.339* / 63.4%* 0.385*/67.9%* 0.278*/60.2%* 76.7%* 62.6%*', file_path='../pdfs/chen2017.pdf', page_num='9,10')\n",
      "TextChunk(text='MinRR h / 70.0% / 61.8%* / / / 61.5%* 0.442*/65.4%* 0.494*/64.4%* 0.464* 0.474* 0.440* 66.7%* 0.462* 67.5%* 0.486* MinRR ch / 70.0% / 61.8%* / / / 61.5%* 0.442*/65.4%* 0.494*/64.4%* 0.464* 0.474* 0.440* 66.7%* 0.462* 67.5%* 0.486* PLC ac 0.408* / 70.0%* 0.195* / 57.2%* 0.181* / 57.9%* 0.242* / 57.7%* 0.213* / 58.5%* 0.366*/61.5% 0.176*/57.7%* PLC sc 0.416* / 73.3%* 0.198* / 56.9%* 0.119* / 57.1%* 0.190* / 57.7%* 0.215* / 58.5%* 0.360*/67.9%* 0.139/57.7%* PLC h 0.302 / 56.7% 0.113 / 48.0% 0.176 / 46.4% 0.198 / 47.6% 0.108 / 47.5% 0.179/51.3% 0.142/47/1% PLC ch / / / / / 0.429*/69.2%* 0.265*/60.4%* 0.444 80.0%* 0.262* 59.2%* 0.302* 61.7%* 0.347* 63.8%* 0.271* 58.7%* educational objectives. Allyn & Bacon. and knowledge discovery 18, 1 (2009), 140181. Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson   Dmitry Lagun, Chih-Hung Hsieh, Dale Webster, and Vidhya Navalpakkam. 2014. Towards beter measurement of atention and satisfaction in mobile search.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='In correlation coefficient. In Noise reduction in speech processing. Springer, 14. Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, Vol. 36.  SIGIR14. ACM, 113122.   ACM, 310. Jane Li, Scot Huffman, and Akihito Tokuda. 2009. Good abandonment in mobile Chris Buckley and Ellen M Voorhees. 2000. Evaluating evaluation measure  and PC internet search. In SIGIR09. ACM, 4350. stability. In SIGIR00. ACM, 3340.   Lihong Li, Jin Young Kim, and Imed Zitouni. 2015. Toward predicting the outcome Stefan B utcher, Charles LA Clarke, Peter CK Yeung, and Ian Soboroff. 2007.   of an A/B experiment for search relevance. In WSDM15. ACM, 3746. Yiqun Liu, Ye Chen, Jinhui Tang, Jiashen Sun, Min Zhang, Shaoping Ma, and  Reliable information retrieval evaluation with incomplete and biased judgements. In SIGIR07. ACM, 6370. Xuan Zhu. 2015. Different users, different opinions: Predicting search satisfaction  with mouse movement information. In SIGIR15. ACM, 493502.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='Ben Carterete, Evangelos Kanoulas, and Emine Yilmaz. 2010. Low cost evaluation in information retrieval. In SIGIR10. ACM, 903903.   Zeyang Liu, Yiqun Liu, Ke Zhou, Min Zhang, and Shaoping Ma. 2015. Influence  of vertical result in web search examination. In SIGIR15. ACM, 193202. Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Xiaolu Lu, Alistair Moffat, and J Shane Culpepper. 2016. Te effect of pooling reciprocal rank for graded relevance. In CIKM09. ACM, 621630.   Ye Chen, Yiqun Liu, Ke Zhou, Meng Wang, Min Zhang, and Shaoping Ma. and evaluation depth on IR metrics. Information Retrieval Journal 19, 4 (2016),  416445. 2015. Does Vertical Bring more Satisfaction?: Predicting Search Satisfaction in a Heterogeneous Environment. In CIKM15. ACM, 15811590.   Jiaxin Mao, Yiqun Liu, Ke Zhou, Jian-Yun Nie, Jingtao Song, Min Zhang, Shaop- ing Ma, Jiashen Sun, and Hengliang Luo. 2016.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='When does Relevance Mean  Aleksandr Chuklin, Pavel Serdyukov, and Maarten De Rijke. 2013. Click model- based information retrieval metrics. In SIGIR13. ACM, 493502. Usefulness and User Satisfaction in Web Search?. In SIGIR16. ACM. Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement   Cyril Cleverdon, Jack Mills, and Michael Keen. 1966. FACTORS DETERMINING THE PERFORMANCE OF INDEXING SYSTEMS VOLUME 1. DESIGN. (1966). of retrieval effectiveness. TOIS08 27, 1 (2008), 2. Alex Deng and Xiaolin Shi. 2016. Data-Driven Metric Development for Online   Vidhya Navalpakkam, LaDawn Jentzsch, Rory Sayres, Sujith Ravi, Amr Ahmed, and Alex Smola. 2013. Measurement and modeling of eye-mouse behavior in Controlled Experiments: Seven Lessons Learned. In SIKDD16. ACM.   the presence of nonlinear page layouts. In WWW13. ACM, 953964. Geoffrey Evans, Anthony Heath, and Mansur Lalljee. 1996. Measuring lef-right and libertarian-authoritarian values in the British electorate.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='British Journal of  Filip Radlinski and Nick Craswell. 2010. Comparing the sensitivity of information Sociology (1996), 93112. retrieval metrics. In SIGIR10. ACM, 667674. Sri Devi Ravana and Alistair Moffat. 2010. Score estimation, incomplete judg-   Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais, and Tomas White. ments, and significance testing in IR evaluation. In AIRS10. Springer, 97109. 2005. Evaluating implicit measures to improve web search. TOIS05 23, 2 (2005), Tetsuya Sakai. 2006. Evaluating evaluation metrics based on the bootstrap. In 147168.   Qi Guo, Dmitry Lagun, and Eugene Agichtein. 2012. Predicting web search  SIGIR06. ACM, 525532. success with fine-grained interaction data. In CIKM12. ACM, 20502054.   Tetsuya Sakai. 2013. How intuitive are diversified search metrics? Concordance  test results for the diversity U-measures. In AIRS13. Springer, 1324. Ahmed Hassan, Xiaolin Shi, Nick Craswell, and Bill Ramsey. 2013.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='Beyond clicks: query reformulation as a predictor of search satisfaction. In CIKM13. ACM,  Tetsuya Sakai and Noriko Kando. 2008. On information retrieval metrics designed 20192028. for evaluation with incomplete relevance assessments. Information Retrieval 11,  5 (2008), 447470. Ahmed Hassan, Yang Song, and Li-wei He. 2011. A task level metric for measuring Mark Sanderson. 2010. Test collection based evaluation of information retrieval web search satisfaction and its application on improving relevance estimation.   In CIKM11. ACM, 125134. systems. Now Publishers Inc. Anne Schuth, Katja Hofmann, and Filip Radlinski. 2015. Predicting search Katja Hofmann, Lihong Li, Filip Radlinski, and others. 2016. Online evaluation   Trends satisfaction metrics with interleaved comparisons. In SIGIR15. ACM, 463472. for information retrieval. Foundations and in Information Retrieval 10, 1 Sch utze. (2016), 1117.   Hinrich 2008. Introduction to Information Retrieval.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='In Proceedings of the Scot B Huffman and Michael Hochster. 2007. How well does result relevance  international communication of association for computing machinery conference. Louise T Su. 1992. Evaluation measures for interactive information retrieval. predict session satisfaction?. In SIGIR07. ACM, 567574.   Jiepu Jiang, Ahmed Hassan Awadallah, Xiaolin Shi, and Ryen W White. 2015.   Information Processing & Management 28, 4 (1992), 503516. Understanding and predicting graded search satisfaction. In WSDM15. ACM,  Louise T Su. 2003. A comprehensive and systematic model of user evaluation of 5766. Web search engines: II. An evaluation by undergraduates. Journal of the American  Society for Information Science and Technology 54, 13 (2003), 11931223. Jiepu Jiang, Daqing He, and James Allan. 2014. Searching, browsing, and clicking Hongning Wang, Yang Song, Ming-Wei Chang, Xiaodong He, Ahmed Hassan,  in a search session: changes in user behavior by task and over time. In SIGIR14.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='and Ryen W White. 2014. Modeling action-level satisfaction for search task ACM, 607616.   satisfaction prediction. In SIGIR14. ACM, 123132. Torsten Joachims. 2002. Optimizing search engines using clickthrough data. In William Webber, Alistair Moffat, and Justin Zobel. 2010. A similarity measure SIGKDD02. ACM, 133142.   for indefinite rankings. TOIS10 28, 4 (2010), 20. Evangelos Kanoulas, Ben Carterete, Paul D Clough, and Mark Sanderson. 2011. Emine Yilmaz, Evangelos Kanoulas, and Javed A Aslam. 2008. A simple and Evaluating multi-query sessions. In SIGIR11. ACM, 10531062.   Diane Kelly. 2009. Methods for evaluating interactive information retrieval efficient sampling method for estimating AP and NDCG. In SIGIR08. ACM,  systems with users. Foundations and Trends in Information Retrieval 3, 1fi!2 603610. Emine Yilmaz and Stephen Robertson. 2010. On the choice of effectiveness  (2009), 1224.', file_path='../pdfs/chen2017.pdf', page_num='10')\n",
      "TextChunk(text='Julia Kiseleva, Kyle Williams, Jiepu Jiang, Ahmed Hassan Awadallah, Imed Zi-  measures for learning to rank. Information Retrieval 13, 3 (2010), 271290. Tom as  touni, Aidan C Crook, and Tasos Anastasakos. 2016. Predicting User Satisfaction Masrour Zoghi, Tunys, Lihong Li, Damien Jose, Junyan Chen, Chun Ming Chin, and Maarten de Rijke. 2016. Click-based Hot Fixes for Underperforming with Intelligent Assistants. In SIGIR16. 495505.   Torso Qeries. SIGIR. Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal M Henne. 2009. Controlled experiments on the web: survey and practical guide. Data mining 24', file_path='../pdfs/chen2017.pdf', page_num='10')\n"
     ]
    }
   ],
   "source": [
    "for chunk in proc.process_text(pdf_path):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
