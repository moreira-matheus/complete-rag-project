{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c4aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabb433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.indexing import IndexingPipeline\n",
    "from model.searching import SearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f2d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REINDEX = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024896e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../cfg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57695cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REINDEX:\n",
    "    pipeline = IndexingPipeline(cfg)\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809791d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Evaluation metrics\"\n",
    "engine = SearchEngine(cfg)\n",
    "results = engine.search_index(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a9706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[\"documents\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11a9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rank\": \"1\",\n",
      "  \"score\": \"0.5952510833740234\",\n",
      "  \"text\": \"beter with user satisfaction. 2 RELATED WORK Of particular interest to our research is the extensive body of work on (i) meta evaluation of IR metrics, and (ii) search satisfaction. 2.1 Meta Evaluation of IR Metrics As evaluation serves as an important part in IR-related research, the meta-evaluation of evaluation metrics has also been widely studied in recent years and different criteria have been adopted to compare different evaluation metrics [33]. One widely-used method is to use discriminative power to measure evaluation metrics. Early in 2000, Buckley and Voorhees proposed to use error rate, which is the likely error of concluding System A is beter than system B, to compare between different metrics [6]. Tey also adopted fuzziness value to examine the power of a measure to discriminate among systems. Tis idea was further formalized to be discriminative power by Sakai in 2006 [39]. He pointed out that mildly top-weighted metrics, such as AP, NDCG and RBP(0.95) usually have higher\",\n",
      "  \"metadata\": \"{'file_path': '../input/raw/chen2017.pdf', 'page_num': 2}\"\n",
      "}\n",
      "{\n",
      "  \"rank\": \"2\",\n",
      "  \"score\": \"0.6742565631866455\",\n",
      "  \"text\": \"that user satisfaction is strongly correlated with some evaluation metrics such as CG and DCG. There exists a number of studies investigating different evaluation methods and the correlation between these methods and satisfaction [28, 31, 38]. Our contributions in this paper complement existing work on thoroughly investigating the consistency between two facets of evaluation metrics, i.e., user behavior prediction and user satisfaction reflection. 2.3 Meta-Evaluation of Metrics The meta-evaluation of evaluation metrics has been widely studied in recent years and different criteria (e.g., error rate, discriminative power and etc.) have been proposed to compare different evaluation metrics. Buckley and Voorhees [7] adopt error rate which is the likely error of concluding System A is better than system B, to compare between different metrics. Sakai [35] propose Discriminative power which shares similar intuitive with fuzziness value proposed in [7]. Discriminative power refers to the\",\n",
      "  \"metadata\": \"{'file_path': '../input/raw/zhang2020.pdf', 'page_num': 3}\"\n",
      "}\n",
      "{\n",
      "  \"rank\": \"3\",\n",
      "  \"score\": \"0.7358291149139404\",\n",
      "  \"text\": \"be publicly available upon publication of the paper. The remainder of this paper is organized as follows. In Section 2, we review a broad range of related studies about user models of metrics, user satisfaction, and meta-evaluation of metrics. Then we describe our methods to meta-evaluate user models and user satisfaction for metrics in Section 3. Section 4 and Section 5 shows some details of our data collection process and experimental settings, respectively. The experiments and results are shown in Section 6 to investigate the relationship between user models and user satisfaction. Finally, we discuss the conclusions and limitations of our work in Section 7. 2 RELATED WORK 2.1 User Models of Metrics Evaluation metrics encapsulate assumptions about user behavior [14, 31]. It has been shown that most advanced evaluation metrics are fundamentally related and are underlied by different user behavior models [9, 30]. Further, within the C/W/L framework proposed by Moffat et al. [31],\",\n",
      "  \"metadata\": \"{'file_path': '../input/raw/zhang2020.pdf', 'page_num': 2}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
