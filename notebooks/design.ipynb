{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c4aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, sys, json, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabb433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.indexing import IndexingPipeline\n",
    "from model.searching import SearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f2d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REINDEX = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024896e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../cfg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57695cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: ../pdfs/zhang2020.pdf\n",
      "File path: ../pdfs/chen2017.pdf\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if REINDEX:\n",
    "    if os.path.exists(cfg[\"INDEX_DIR\"]):\n",
    "        shutil.rmtree(cfg[\"INDEX_DIR\"])\n",
    "\n",
    "    pipeline = IndexingPipeline(cfg)\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809791d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Evaluation metrics\"\n",
    "engine = SearchEngine(cfg)\n",
    "results = engine.search_index(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a9706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[\"documents\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11a9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rank\": \"1\",\n",
      "  \"score\": \"0.6531605124473572\",\n",
      "  \"text\": \"ation metric should not only correspond to a user model which ACM ISBN 978-1-4503-8016-4/20/07...$15.00 can accurately predict user behavior, but also correlate well with https://doi.org/10.1145/3397271.3401162 379 Session 2C: Evaluation SIGIR 20, July 2530, 2020, Virtual Event, China 2 RELATED WORK user satisfaction. However, to our best knowledge, few studies have explored the consistency between these two facets of eval- 2.1 User Models of Metrics uation metrics. A recent work  attempts to construct a novel Evaluation metrics encapsulate assumptions about user behav- framework which considers the accuracy of user model and the cor- ior [14, 31]. It has been shown that most advanced evaluation relation with user satisfaction for meta-evaluation of metrics. They metrics are fundamentally related and are underlied by different show how the accuracy of user model and the correlation between [9, 30].\",\n",
      "  \"metadata\": \"{'file_path': '../pdfs/zhang2020.pdf', 'page_num': '1,2'}\"\n",
      "}\n",
      "{\n",
      "  \"rank\": \"2\",\n",
      "  \"score\": \"0.6667564511299133\",\n",
      "  \"text\": \"Therefore, the validity standing of Evaluation Metrics. In Proceedings of the 43rd International ACM of an evaluation metric has two facets: whether the underlying SIGIR Conference on Research and Development in Information Retrieval (SI- user model can accurately predict user behavior and whether the GIR 20), July 2530, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401162 evaluation metric correlates well with user satisfaction. While a tremendous amount of work has been undertaken to design, eval- uate, and compare different evaluation metrics, few studies have 1 INTRODUCTION explored the consistency between these two facets of evaluation As batch evaluation plays a central part in IR research, how to metrics. Specifically, we want to investigate whether the metrics design and meta-evaluate different evaluation metrics have been that are well calibrated with user behavior data can perform as widely studied for many years.\",\n",
      "  \"metadata\": \"{'file_path': '../pdfs/zhang2020.pdf', 'page_num': '1'}\"\n",
      "}\n",
      "{\n",
      "  \"rank\": \"3\",\n",
      "  \"score\": \"0.6927136182785034\",\n",
      "  \"text\": \"evaluation metrics, i.e., user behavior prediction and user satisfac- tion reflection. Different from probability distributions defined by user models, C(), W L() (), and need to be estimated from observed user behav- C(), W ior. Following previous research , we estimate (), and 2.3 Meta-Evaluation of Metrics L() as follows: The meta-evaluation of evaluation metrics has been widely studied P(view in recent years and different criteria (e.g., error rate, discriminative = i + 1|u,q) P P u U qQ(u) C(i) = (1) power and etc.) have been proposed to compare different evalua- P(view = i|u,q) P P u U qQ (u) tion metrics. Buckley and Voorhees adopt error rate which  P(view is the likely error of concluding System A is better than system = i|u,q) P P u U qQ(u) W (i) = (2)  B, to compare between different metrics. Sakai propose Dis- P(view PN = j|u,q) P P u U qQ (u) j=1 criminative power which shares similar intuitive with fuzziness P(view P(view = i|u,q) = i + 1|u,q) .\",\n",
      "  \"metadata\": \"{'page_num': '3', 'file_path': '../pdfs/zhang2020.pdf'}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
